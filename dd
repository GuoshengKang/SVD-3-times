作者：覃含章
链接：https://www.zhihu.com/question/38319536/answer/131029607
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

首先我们定义PCA的有关概念。一种对PCA的核心意图的解释是，找到另一组正交基，使得进行变换后的方差（variance）最大（因为选取了方差最大的维度，所以这样可以存储最多的信息）。 如下图，如果两个维度之间的data有强相关的话这两个维度的数据会趋近一条直线（这意味着其中一个维度的数据是多余的），反之则会有比较大的variance。记作变换后的矩阵为。其中，矩阵的行就组成了主元（principal components）。&lt;img src="https://pic3.zhimg.com/50/v2-c571ae25a3556017594aaa12a6ebc309_hd.jpg" data-rawwidth="475" data-rawheight="391" class="origin_image zh-lightbox-thumb" width="475" data-original="https://pic3.zhimg.com/v2-c571ae25a3556017594aaa12a6ebc309_r.jpg"&gt;实际中最好的做法是选择一个合适的，使得的协方差矩阵（covariance matrix）能够被对角化(diagonalized)。这是符合直观的因为这样子所有的covariance都被消除了（比如可能本来有很多noise）而留下的就是最能体现信息量的方差本身。具体的做法则是，注意到也是对称正定矩阵所以我们可以做特征值分解（eigen-decomposition）得到,其中是对角矩阵（对角元是特征值），的每列是相应的特征向量。我们令便能得到我们的principal components。因为用这样的，我们就有（注意因为是正交阵，所以）:即我们可以将对角化。而SVD来源于另外的一套数学概念，不过我们将要说明这套概念和PCA是内在关联的。不同于特征值分解，SVD（奇异值分解）可以作用于任何形状的矩阵。于是我们则定义对的SVD为，其中是两个正交阵而是对角阵（对角元是的奇异值，即singular values）。我们由此也可以看到SVD比特征值分解要强得多的泛用性，这也是它广泛被用于数值计算的原因。那么它与PCA的关系呢？我们考虑的SVD表示方式：,所以到这里答案就很明显了，我们只需要取另一个投影矩阵就可以将对角化，即的列是principal components。顺便，我们得到了一个副产品奇异值和特征值的关系：，其中是和相应的特征值和奇异值。因此，我们得到了SVD是PCA的另一种algebraic formulation。而这也提供了另外一种算法来计算PCA，实际上，平时我就是用SVD定义的这套算法来做PCA的。因为很方便，计算一次就可以了。额外补充一点，经常我们希望用PCA对进行压缩，比如只保留维度的数据，这个时候我们只需要仅保留的前列（前个principal components），记作，然后就是我们所要的压缩后的数据。
